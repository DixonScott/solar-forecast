{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from build_dataset import data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/The_Dataset_v1-1.csv', parse_dates = [\"Date\"])\n",
    "df = data_cleaning.clean_dataset(df)\n",
    "df = data_cleaning.remove_systems(df)\n",
    "df = data_cleaning.weather_code_to_category(df)\n",
    "df = df.drop(columns=['Date', 'System ID'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into testing and training data:\n",
    "X = df.drop('Efficiency (kWh/kW)', axis=1)\n",
    "y = df['Efficiency (kWh/kW)']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Start with a linear regression with forward subset selection.\n",
    "lin_reg = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(lin_reg, direction=\"forward\", cv=5, n_jobs=-1)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Predict with selected features\n",
    "X_train_sfs = sfs.transform(X_train)\n",
    "X_test_sfs = sfs.transform(X_test)\n",
    "lin_reg.fit(X_train_sfs, y_train)\n",
    "y_pred_sfs = lin_reg.predict(X_test_sfs)\n",
    "rmse_sfs = root_mean_squared_error(y_test, y_pred_sfs)\n",
    "r2_sfs = r2_score(y_test, y_pred_sfs)\n",
    "\n",
    "# Print the results\n",
    "print(\"Forward Subset Selection Linear Regression:\")\n",
    "print(f\"RMSE: {rmse_sfs:.4f}\")\n",
    "print(f\"R²:   {r2_sfs:.4f}\")\n",
    "\n",
    "\n",
    "# Also try an Elastic Net\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"elastic\", ElasticNet(max_iter=10000))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    \"elastic__alpha\": np.logspace(-3, 1, 100),\n",
    "    \"elastic__l1_ratio\": np.linspace(0, 1, 100)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred_en = random_search.predict(X_test)\n",
    "rmse_en = root_mean_squared_error(y_test, y_pred_en)\n",
    "r2_en = r2_score(y_test, y_pred_en)\n",
    "\n",
    "print(\"\\nElastic Net Regression (Random Search):\")\n",
    "print(f\"RMSE: {rmse_en:.4f}\")\n",
    "print(f\"R²:   {r2_en:.4f}\")\n",
    "print(f\"Best Params: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The elastic net shows more lasso regression than ridge regression (l1_ratio is closer to 1 than 0).\n",
    "# RMSE is 0.87 in both cases. This has units of hours (efficiency is daily energy production (kWh) / power rating (kW)),\n",
    "# so it indicates an average prediction error of 0.87 * 60 = 52 minutes.\n",
    "\n",
    "# Let's see which features were selected by forward subset selection\n",
    "subset_coefs = sfs.get_support().astype(int)\n",
    "\n",
    "# View the coefficients for the elastic net:\n",
    "best_pipeline = random_search.best_estimator_\n",
    "elastic_model = best_pipeline.named_steps[\"elastic\"]\n",
    "elastic_coefs = elastic_model.coef_\n",
    "intercept = elastic_model.intercept_\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Elastic Coefficient\": elastic_coefs,\n",
    "    \"Selected by FSS?\": subset_coefs\n",
    "})\n",
    "\n",
    "# Sort by the elastic net coefficient\n",
    "coef_df[\"|Elastic Coefficient|\"] = coef_df[\"Elastic Coefficient\"].abs()\n",
    "coef_df = coef_df.sort_values(by=\"|Elastic Coefficient|\", ascending=False).drop(columns=\"|Elastic Coefficient|\")\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortwave_radiation_sum has a much higher coefficient than all other variables.\n",
    "# Try a random forest.\n",
    "\n",
    "# There will be multiple random forests so let's write a function:\n",
    "def run_rf_search(X_train, y_train, X_test, y_test, param_dist, n_iter = 20):\n",
    "    # Time it\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(\"Start time:\", current_time)\n",
    "        \n",
    "    # Initialize model\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=1)\n",
    "    \n",
    "    # Randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        verbose=2,\n",
    "        n_jobs=8,\n",
    "        random_state=42,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Run the search\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_rf = random_search.best_estimator_\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Best RF Test RMSE:\", rmse)\n",
    "    print(\"Best RF Test R²:\", r2)\n",
    "    \n",
    "    # Show top CV results\n",
    "    cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "    cv_results = cv_results.sort_values(by='mean_test_score', ascending=False)\n",
    "    print(\"\\nCV Results:\")\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    display(cv_results[['mean_test_score', 'std_test_score', 'params']])\n",
    "    current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "    cv_results.to_csv(f'random_forest_cv_results_{current_time}.csv')\n",
    "    \n",
    "    # Print end time\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(\"End time:\", current_time)\n",
    "    \n",
    "    return best_rf, random_search\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(10, 50),\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'max_features': ['sqrt', None]\n",
    "    }\n",
    "\n",
    "# Run it with 20 iterations (takes over an hour).\n",
    "# Commented out to prevent accidental execution.\n",
    "# Run a small random forest so that code in the following cells works, although comments interpret results from the full search.\n",
    "# Uncomment the final line of the cell and run for the full random forest search.\n",
    "best_rf, random_search = run_rf_search(X_train, y_train, X_test, y_test, {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [10],\n",
    "        'min_samples_leaf': [2],\n",
    "        'max_features': ['sqrt']\n",
    "    }, n_iter = 1)\n",
    "# best_rf, random_search = run_rf_search(X_train, y_train, X_test, y_test, param_dist, n_iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small improvement in RMSE. The linear regression gave 52 minutes, and the random forest gives 48 minutes (RMSE is 0.8 hours).\n",
    "# Changing hyperparameters has little effect on the performance.\n",
    "\n",
    "# View the importances of the features:\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns)\n",
    "importances.sort_values().plot(kind='barh', figsize=(10, 6), title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is likely that there is little improvement to be made on 0.8 RMSE without:\n",
    "# collecting more data\n",
    "# Improving the quality of data: e.g. some systems on pvoutput.org record the weather condition and temperature.\n",
    "#     So a really high quality dataset would only include these systems, and only include those days where the pvoutput weather matches the historical forecast.\n",
    "# Taking into account the specific hardware used in each solar panel system,\n",
    "#     because that will be causing some variance in solar panel output given the same weather conditions\n",
    "# Only including those days where the solar panel system was operational for the entire duration of daylight\n",
    "\n",
    "# given my time constraints, I will try the following:\n",
    "# 1. add elevation above sea level to the dataset.\n",
    "# 2. try engineering some new variables\n",
    "# 3. try removing the weather category dummy variables, not because it would improve the model, but because they are unimportant and would make the model simpler\n",
    "\n",
    "# Load dataset with elevation variable\n",
    "df = pd.read_csv('../data/The_Dataset_v1-2.csv', parse_dates = [\"Date\"])\n",
    "df = data_cleaning.clean_dataset(df)\n",
    "df = data_cleaning.remove_systems(df)\n",
    "df = data_cleaning.weather_code_to_category(df)\n",
    "df = df.drop(columns=['Date', 'System ID'])\n",
    "\n",
    "# Plot efficiency vs elevation\n",
    "# Control for weather by filtering for some weather codes:\n",
    "df_clear = df[(df.filter(like='weather_category') == 0).all(axis=1)]\n",
    "df_partly_cloudy = df[df['weather_category_partly_cloudy'] == 1]\n",
    "df_overcast = df[df['weather_category_overcast'] == 1]\n",
    "\n",
    "# Plot the filtered data\n",
    "plt.scatter(df_clear['Elevation (m)'], df_clear['Efficiency (kWh/kW)'])\n",
    "plt.title('Clear')\n",
    "plt.xlabel('Elevation')\n",
    "plt.ylabel('Efficiency (kWh/kW)')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Partly Cloudy\n",
    "plt.scatter(df_partly_cloudy['Elevation (m)'], df_partly_cloudy['Efficiency (kWh/kW)'], color='orange')\n",
    "plt.title('Partly Cloudy')\n",
    "plt.xlabel('Elevation')\n",
    "plt.ylabel('Efficiency (kWh/kW)')\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Overcast\n",
    "plt.scatter(df_overcast['Elevation (m)'], df_overcast['Efficiency (kWh/kW)'], color='gray')\n",
    "plt.title('Overcast')\n",
    "plt.xlabel('Elevation')\n",
    "plt.ylabel('Efficiency (kWh/kW)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No obvious correlation between elevation and efficiency, though a random forest might still show an improvement.\n",
    "\n",
    "# Start by doing a quick test using the already trained random forest:\n",
    "X = df.drop('Efficiency (kWh/kW)', axis=1)\n",
    "y = df['Efficiency (kWh/kW)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "best_rf = RandomForestRegressor(**best_rf.get_params())\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Test set evaluation\n",
    "y_pred = best_rf.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best RF Test RMSE:\", rmse)\n",
    "print(\"Best RF Test R²:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A minor improvement, even using an untuned random forest model.\n",
    "# Let's see if that improvement is robust by running a new random search (with fewer hyperparameter combinations):\n",
    "best_rf_e, random_search_e = run_rf_search(X_train, y_train, X_test, y_test, param_dist, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RMSE has decreased again after running the search again. Clearly, elevation is an important variable, and\n",
    "# the importances plot should confirm this:\n",
    "importances = pd.Series(best_rf_e.feature_importances_, index=X.columns)\n",
    "importances.sort_values().plot(kind='barh', figsize=(10, 6), title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elevation is the second most important feature.\n",
    "# Strangely, \"shortwave_radiation_sum\" now has far greater relative importance compared to the other features.\n",
    "# This can be explained by looking at the hyperparameters of the two random forests trained so far:\n",
    "print(best_rf.get_params())  # random forest on the dataset without elevation\n",
    "print(best_rf_e.get_params())  # with elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the first random forest has 'max_features': 'sqrt', the new random forest has 'max_features': 'None'.\n",
    "# This means that each decision tree in the new random forest was allowed to check through every variable to find the optimum split,\n",
    "# so it was free to choose 'shortwave_radiation_sum' for every split (as opposed to sqrt(n) of the splits where there are n variables).\n",
    "\n",
    "# Let's run a few more random forests with different available features.\n",
    "# To reiterate: I will try engineering some new variables, and removing the weather category dummy variables.\n",
    "\n",
    "# Use a slightly smaller parameter distribution:\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 300),\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt']  # To more clearly see the relative importances of features\n",
    "}\n",
    "\n",
    "df_new = df.copy()\n",
    "# Create some potentially useful new features\n",
    "df_new['radiation_strength'] = df_new['shortwave_radiation_sum'] / df_new['daylight_duration']\n",
    "df_new['sunshine_ratio'] = df_new['sunshine_duration'] / df_new['daylight_duration']\n",
    "df_new['precipitation_intensity'] = df_new['precipitation_sum'] / df_new['precipitation_hours']\n",
    "\n",
    "# transform wind direction\n",
    "wind_rad = np.deg2rad(df_new['wind_direction_10m_dominant'])\n",
    "df_new['wind_dir_sin'] = np.sin(wind_rad)\n",
    "df_new['wind_dir_cos'] = np.cos(wind_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code from here takes about half an hour to run. It is not necessary to run this code for the remaining cells to run.\n",
    "\n",
    "# Training the random forests\n",
    "# without weather category\n",
    "X = df.drop(columns=[col for col in df.columns if col.startswith('weather_category_')] + ['Efficiency (kWh/kW)'])\n",
    "y = df['Efficiency (kWh/kW)']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "best_rf_no_code, random_search_no_code = run_rf_search(X_train, y_train, X_test, y_test, param_dist, n_iter = 10)\n",
    "\n",
    "importances = pd.Series(best_rf_no_code.feature_importances_, index=X.columns)\n",
    "importances.sort_values().plot(kind='barh', figsize=(10, 6), title=\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# with new variables\n",
    "X = df_new.drop('Efficiency (kWh/kW)', axis=1)\n",
    "y = df_new['Efficiency (kWh/kW)']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "best_rf_new, random_search_new = run_rf_search(X_train, y_train, X_test, y_test, param_dist, n_iter = 10)\n",
    "\n",
    "importances = pd.Series(best_rf_new.feature_importances_, index=X.columns)\n",
    "importances.sort_values().plot(kind='barh', figsize=(10, 6), title=\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# with new variables without weather category\n",
    "X = df_new.drop(columns=[col for col in df.columns if col.startswith('weather_category_')] + ['Efficiency (kWh/kW)'])\n",
    "y = df_new['Efficiency (kWh/kW)']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "best_rf_new_no_code, random_search_new_no_code = run_rf_search(X_train, y_train, X_test, y_test, param_dist, n_iter = 10)\n",
    "\n",
    "importances = pd.Series(best_rf_new_no_code.feature_importances_, index=X.columns)\n",
    "importances.sort_values().plot(kind='barh', figsize=(10, 6), title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of the models show any significant improvement in predictive power. Stick with the original dataset plus elevation.\n",
    "\n",
    "# Now is a good time for an overnight hyperparameter search.\n",
    "param_dist = {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(20, 50),\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', None]\n",
    "    }\n",
    "\n",
    "X = df.drop('Efficiency (kWh/kW)', axis=1)\n",
    "y = df['Efficiency (kWh/kW)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Commented out to prevent accidental execution. It takes several hours to run.\n",
    "'''\n",
    "best_rf, search = run_rf_search(X_train, y_train, X_test, y_test, param_dist, n_iter = 200)\n",
    "\n",
    "joblib.dump(best_rf, 'rf_big_search.pkl')\n",
    "joblib.dump(search, 'big_search.pkl')\n",
    "'''\n",
    "# Instead I will load in the results from a csv:\n",
    "hyperparam_search_results = pd.read_csv(\"large_hyperparam_search.csv\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(hyperparam_search_results[['mean_test_score', 'std_test_score', 'params']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest performing models have large values for n_estimators and max_depth.\n",
    "# However, the highest perfoming model has a very large file size after saving it with joblib.\n",
    "# This will be an issue for deploying the model in my web app.\n",
    "\n",
    "# The performance drops off very slowly and there is a negligible difference between the highest-performing models\n",
    "# and models with max_features = None, min_samples_leaf = 2, and much smaller max_depth and n_estimators.\n",
    "# So let's train a smaller forest and presume that it will perform nearly as well as the \"best\" model, but with a much smaller file size.\n",
    "\n",
    "params = {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [20],\n",
    "        'min_samples_leaf': [2],\n",
    "        'max_features': [None]\n",
    "    }\n",
    "small_rf, small_search = run_rf_search(X_train, y_train, X_test, y_test, param_dist = params, n_iter = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE is 0.0048 hours (17.28 seconds) greater on this much smaller forest,\n",
    "# a negligible loss of accuracy for the huge reduction in file size and memory usage.\n",
    "\n",
    "joblib.dump(small_rf, 'rf_100.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (solar_power venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
